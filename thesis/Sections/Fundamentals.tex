\chapter{Fundamentals and related work}

\section{Software-Defined Networking}

The origin of Software-Defined Networking (SDN) began already in 1995, however the first use cases were only developed in 2001 and the promotion of SDN only began with the foundation of the non-profit industry consortium Open Networking Foundation (ONF) in 2011. % % https://en.wikipedia.org/wiki/Software-defined_networking % %
The ONF is dedicated to push and adapt open standards like the OpenFlow into the industry.
In this following section a brief overview of the SDN architecture and concepts, including the OpenFlow protocol is given.

\subsection{Motivation}

Today's internet is part of the modern society, be it for private users, enterprises or vital infrastructure services. Networks are required to evolve in order to address the challenges that are entailed with new applications, services and a growing number of end-users.

With a more detailed view on the challenges of current networks one comes to see the following limitations:

% % ONF White Paper (pdf): SDN - The new norm for networks % % 
\begin{itemize}
\item \textbf{Inability to scale}: With the expansion of data centers, networks must grow too. Configuring and managing these additional network devices comes at a high administrative effort. With the virtualization of data centers network traffic patterns becomes more and more dynamic and unpredictable. With multi-tenancy a further complication is introduced, because different end-users and services need different network performance and might require traffic steering. Such scaling and network management cannot be done with a manual configuration of the underlying infrastructure.
\item \textbf{Complexity}: In the past decades new networking protocols have been adapted by the industry. To add or move any device, multiple existing switches, routes, firewalls must be touched in order to manage protocol-based mechanisms on a device-level. With the virtualization of servers the amount of interfaces that need network connectivity and the distribution of applications over a number of virtual machines (VMs) are another demand that the current fairly static networks cannot dynamically adapt to.
\item \textbf{Inconsistent policies}: For IT to apply a network- or data center-wide policy a lot of devices and mechanisms may need to be reconfigured. Virtual Machines are created and rebuilt within no time, but if for example access or security needs to be updated, the benefits of this dynamic are subverted(?).
\item \textbf{Vendor dependence}: Standards are needed to match the requirements of the markets with the capabilities of networks and enable network operators to customize the network to specific environments.
\end{itemize}

Traditionally decisions about traffic flowing through the network are made directly by each network device, because the control logic and forwarding hardware are tightly coupled.

\subsubsection{Classical switches \& routers}

Packet forwarding (data plane) and routing decisions (control plane) in classical switching and routing are both within one device. In figure .. the main components that are depicted have the following functions:
\begin{enumerate}
\item The \textbf{forwarding path} typically handles data path operations for each packet. It generally consists of Application-Specific Integrated Circuits (ASIC), network-processors or general-purpose processors that forwards frames and packets at wire speed (line-rate). Their lookup functions can be further increased with memory resources like Content Addressable Memory (CAM) or Ternary Content Addressable Memory (TCAM) to contain the forwarding information.
\item The elements in the \textbf{control plane} are based on general-purpose processors that provide services like routing and signaling protocols, including ARP, MAC Learning and forwarding tables.
\end{enumerate}

\begin{figure}[H]
\centering

\includegraphics[width=0.5\textwidth]{images/fundamentals/switch_components}

\caption{"Classical" switch components}
\end{figure}

A switch consists of multiple ports for incoming and outgoing data. Internal forwarding tables classify the packets and forward them to one or many specific ports. It does so by collecting MAC addresses and storing their corresponding port in specific tables. Layer 2 switches also support the segregation into virtual LANs (VLAN), which enables the network operator to logically isolate networks that share a single switch.

Routers forward packets on the Network layer (Layer 3) and routing-decisions are made based on IP addresses. They contain a routing table where paths to neighbour networks are stored, so that packets can be forwarded to their destination IP address. Other features that can be configured with routers are Quality of Service (QoS), Network Address Translation (NAT) and packet filtering.

The main differences between the classical architecture and SDN will be further described in the coming sections.

\subsection{Software-Defined Networking concept}

%% https://www.opennetworking.org/sdn-resources/sdn-definition % %

SDN represents a new dynamic, manageable, cost-effective and adaptable architecture that is built to serve the dynamic infrastructures that are needed as a backbone for today's data centers. Opposed to the traditional approach, network control and forwarding functions are decoupled and thus can be programmed and divided into different applications and network services.
The work of the Open Networking Foundation laid out the OpenFlow protocol as the base for modern SDN solutions.

\subsection{SDN architecture}

SDN separates the architecture into three distinct layers that communicate with each other through different APIs. In figure .. this separation is shown.

\begin{itemize}
\item \textbf{Infrastructure Layer:} here all physical and virtual devices (e.g. switches and routers) that are capable of the OpenFlow Protocol provide forwarding mechanisms on different Network Layers.
\item \textbf{Control Layer:} represents the 'network intelligence' and collects global view of the network, by communicating with the switching elements through the so called Southbound API.
\item \textbf{Application Layer:} consists of business applications that allow the network operator to extend the SDN controller on an abstracted level, without being tied to the actual details of the implementation of the infrastructure. This communication with the Control Layer


\end{itemize}

\begin{figure}[H]
\centering

\includegraphics[width=0.5\textwidth]{images/fundamentals/sdn_logical_architecture.png}

\caption{Software-Defined Network architecture}
\end{figure}

\subsection{OpenFlow}

% % https://www.opennetworking.org/sdn-resources/openflow % %

With OpenFlow the Open Networking Foundation defined the first standard communications interface between the SDN architecture's control and forwarding layers. It enables manipulation and direct access to the forwarding plane of physical as well as virtual (hypervisor-based) network devices such as switches and routers.

\begin{figure}[H]
\centering

\includegraphics[width=0.6\textwidth]{images/fundamentals/openflow_architecture.png}

\caption{OpenFlow Network Architecture}
\end{figure}

OpenFlow first of all stands for the communications protocol that is used by SDN controllers to fetch information and configure switches. Additionally it is a switch specification that defines its minimum capabilities in order to support OpenFlow.

% % Vendor OF version usage: http://www.tomsitpro.com/articles/pica8-openflow-1.4-sdn-switches,1-1927.html % %

Most of the OpenFlow-enabled switches and controllers currently still only support the OpenFlow version 1.0 (released in December 2009). The newest version at this date is 1.4, however this explanation of OpenFlow will be focussed on version 1.3 since that is the most recent specification which is supported by OpenVSwitch.
% %http://sdnhub.org/tutorials/openflow-1-3/ % %
The main features added since version 1.0 are among others support for VLANs, IPv6, tunnelling and per-flow traffic meters.

Generally the switches are backwards-compatible down to version 1.0. In the following description the focus lies on the required features of all OpenFlow capable devices, however it has to be mentioned that there is also a set of optional features.

\subsubsection{OpenFlow Controller}

The OpenFlow controller is separated from the switch and has two interfaces. The northbound interface is an API to the application layer for implementing applications that control the network. The southbound interface connects with the underlying switches using the OpenFlow protocol.

\subsubsection{OpenFlow Switch}
% %PDF: OpenFlow spec 1.3 % %

There are two varieties of OpenFlow-compliant switches:
\begin{itemize}
\item \textbf{OpenFlow-only:} in these switches all packets are processed by the OpenFlow pipeline and they have no legacy features.
\item \textbf{OpenFloy-hybrid:} support OpenFlow and normal Ethernet switching (including traditional L2 Ethernet switching, VLAN isolation, L3 routing, ACL and QoS). Most of the commercial switches that are available on the market today are this type.
\end{itemize}

An OpenFlow switch includes one ore multiple flow tables and a group table, which have the function of carrying out packet lookups and forwarding. Another component is the OpenFlow channel to the external controller.

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{images/fundamentals/openflow_switch_components.png}
\caption{OpenFlow Switch components}
\end{figure}

Through the connection using the OpenFlow protocol, it is possible for the controller to add, update and delete flow entries in flow tables.  This action can be performed either reactively or proactively. Sets of flow entries are stored in each flow table and each flow entry consists of \textit{match fields}, \textit{counters}, and a set of \textit{instructions} used for matching packets. (see OF Tables section)

The matching of flow entries begins at the first flow table, however it may continue to additional flow tables, and it uses the first matching entry from each table and performs the instruction that is linked with that specific entry. For packets without any matches a table-miss flow entry can be configured. Flow entries are usually forwarded to a physical port.

The instructions can either include actions  or modify pipeline processing. Packet forwarding, packet modification and group table processing are the possible actions. With pipeline processing packets can be permitted to be sent to other tables for further processing and metadata can be exchanged between tables.

Packets can also be directed to a group, which contains a set of actions for flooding and more complex forwarding semantics (e.g. multipath, fast reroute and link aggregation).

\subsubsection{OpenFlow Ports}
OpenFlow ports are the network interfaces used for passing packets between OpenFlow processing and the rest of the network.
There are various types of ports that are supported by OpenFlow. This section will give a short overview about this port abstraction.
Incoming OpenFlow packets enter the switch on an ingress port, are then processed by the OpenFlow pipeline and forwarded to an output port. (See OF Tables figure for processing).

There are three types of OpenFlow ports that must be supported by an OpenFlow switch:
\begin{itemize}
\item \textbf{Physical ports:} are hardware interfaces on a switch.
\item \textbf{Logical ports:} don't directly interact  with a hardware interface.
\item \textbf{Reserved ports:} contain generic forwarding actions (e.g. sending to the controller, flooding or forwarding using traditional switch processing)
\end{itemize}

\subsubsection{OpenFlow Tables}

\textbf{\underline{Pipeline Processing}}

The OpenFlow pipeline defines specifies how packets correspond with each of the flow tables. 

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/fundamentals/openflow_pipeline_processing.png}
\caption{OpenFlow pipeline processing}
\end{figure}

As illustrated in the figure, each packet is matched against the flow entries starting at the first flow table, called flow table 0. The outcome of the match then decides if other of the sequentially numbered tables may be used. In the following sections the components of the Flow table, the matching procedures and different instructions will be described.


\textbf{\underline{Flow Table}}

A flow table contains flow entries which consist of the following fields:

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline Match Fields & Priority & Counters & Instructions & Timeouts & Cookie \\ 
\hline 
\end{tabular} 
\end{center}

\begin{itemize}
\item \textbf{match fields:} ingress port, packet headers and optionally metadata
\item \textbf{priority:} set the priority of the flow entry
\item \textbf{counters:} is updated for matching packets
\item \textbf{instructions:} to alter the action set or pipeline processing
\item \textbf{timeouts:} set maximum amount of time or idle time before expiration of the flow
\item \textbf{cookie:} is a opaque data value chosen by the controller
\end{itemize}

Each flow table entry is uniquely identifiable by its match fields and priority. 


\underline{\textbf{Packet Matching}}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/fundamentals/openflow_packet_matching.png}
\caption{Packet flow through an OpenFlow switch}
\end{figure}

On a packet's arrival at the Flow Table, the packet match fields are extracted and used for the table lookup. They include different packet header fields. Additionally matches can be made against the ingress port and metadata fields.
If the values in the packet match fields equate  only the flow entry with the highest priority is selected. The associated counters are updated and the instruction set applied.

When the instruction set associated with a matching flow entry does not specify a next table, the pipeline processing stops. Only then the packet is processed with it's action set and in most cases forwarded. as shown in Figure 2.6.
However, if the lookup phase does not match any of the entries, a table-miss event occurs.


\underline{\textbf{Table-miss}}

Each flow table must support a table-miss flow entry which specifies how to process packets that are unmatched by other flow entries. The instructions associated with this entry are very alike to any other flow entries, packets are either forwarded to other controllers, dropped or it is continued with the next flow table. In case the table-miss flow entry is non-existent unmatched packets are dropped by default. 


\underline{\textbf{Group Tables}}

A group table consists of group entries and it provides a way to direct the same set of actions as part of action buckets to multiple flows. A flow entry is pointed to a group and enables additional methods of forwarding (e.g. broadcast or multicast).


\underline{\textbf{Meter Tables}}

Meters are on a per-flow level and allow OpenFlow to implement various QoS operations, such as rate-limiting, but it can also be combined with per-port queues to implement more complex QoS like DiffServ.

The main components of a meter entry in the meter table are:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline Meter Identifier & Meter Bands & Counters \\ 
\hline 
\end{tabular} 
\end{center}

\begin{itemize}
\item \textbf{meter identifier:} a 32 bit unsigned integer uniquely identifying the meter
\item \textbf{meter bands:} each meter band specifies the rate of the band and the action that is triggered by exceeding the limit
\item \textbf{counters:} is updated when packets are processed by the meter
\end{itemize}

The rate of packets assigned to a meter are measured and controlled. Meters are directly attached to flow entries, as opposed to queues that are attached to ports. A meter is able to have one or more meter bands, each of which specifies the rate and the way packets should be handled. If the current measured meter rate reached the rate-limit, the band applies an action. 

A meter band is identified by its rate and consists of the following fields:

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline Band Type & Rate & Counters & Type specific arguments \\ 
\hline 
\end{tabular} 
\end{center}

\begin{itemize}
\item \textbf{band type:} defines how the packets are processed
\item \textbf{rate:} selects the meter band for the meter and defines the lowest rate at which the band can apply
\item \textbf{counters:} is updated when packets are processed by the meter
\item \textbf{type specific arguments:} some band have optional arguments
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/fundamentals/openflow_qos.png}
\caption{OpenFlow QoS as a meter}
\end{figure}


\underline{\textbf{Instructions}}

Instructions are executed when a packet matches the flow entry. Their result is either a change to the packet, action set and/or pipeline processing. There are different instruction types and some of them are required for an OpenFlow-enabled switch whereas others are optional:

\begin{itemize}
\item \textbf{Meter \textit{meter\_id}:} direct packet to the specified meter. The packet may be discarded as the result of the metering.
\item \textbf{Apply-Actions \textit{action(s)}:} Applies the specific action(s) instantly, without any change to the Action Set.
\item \textbf{Clear-Actions:} Immediately clears all the actions in the action set.
\item \textbf{Write-Actions \textit{action(s)}:} Merges the specified action(s) into the current action set.
\item \textbf{Write-Metadata \textit{metadata / mask}:} Writes the masked metadata value into the metadata field.
\item \textbf{Goto-Table \textit{next-table-id}:} Indicates the next table in the processing pipeline.
\end{itemize}

A maximum of one instruction of each type is associated with a flow entry and they are executed in the order as specified by the given list. Flow entries can also be rejected if the switch is not able to execute its instruction.


\underline{\textbf{Action Set}}

An action set is associated with each packet, which is empty by default. The action set can be modified using a \textit{Write-Action} or a \textit{Clear-Action} instruction. If there is no \textit{Goto-Table} instruction within the instruction set of a flow entry the pipeline processing is halted and the actions in the action set of the packet are executed.


\underline{\textbf{Actions}}

The following action types are available on OpenFlow-enabled switches:
\begin{itemize}
\item \textbf{Output:} A packet is forwarded to a specified OpenFlow port.
\item \textbf{Set-Queue:} Sets the queue id for a packet. This id helps determining which queue attached to this port is used for scheduling and forwarding the packet when the packet is forwarded to a port using the output action. This forwarding behaviour allows to enable basic QoS support.
\item \textbf{Group:} Process the packet through the specified group.
\item \textbf{Push-Tag/Pop-Tag:} The ability to push/pop tags such as VLAN.
\item \textbf{Set-Field:} Modifies the values of header fields in a packet.
\item \textbf{Change-TTL:} Set the values of IPv4 TTL, IPv6 Hop Limit or MPLS TTL in a packet.
\end{itemize} 

\subsection{Open vSwitch}
% %http://git.openvswitch.org/cgi-bin/gitweb.cgi?p=openvswitch;a=blob_plain;f=FAQ;hb=HEAD % %

\subsubsection{Concept \& Functionality}

Open vSwitch (OVS) is open source software switch that is used in virtualized server environments. It is able to forward traffic traffic between Virtual Machines (VMs) and the physical network, as well as between different VMs on the same physical host. It can be controlled using OpenFlow and the OVSDB management protocol. It can run on any Linux-based virtualization platform i.e. KVM, VirtualBox, XEN, ESXi and is part of the mainline kernel as of Linux 3.3 but can run on kernel 2.6.32 and newer.

% %https://github.com/openvswitch/ovs/blob/master/README.md % %
OVS supports the following features:
\begin{itemize}
\item 802.1Q VLAN model
\item Link Aggregation Control Protocol (LACP)
\item GRE and VXLAN tunneling
\item fine-grained QoS control
\item OpenFlow
\item per VM interface traffic policing
\item High-performance forwarding using a Linux kernel module
\end{itemize}

The internals of OVS are as follows:
% %https://events.linuxfoundation.org/sites/events/files/slides/OVS-LinuxCon%202013.pdf % %
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/fundamentals/openvswitch_architecture.png}
\caption{Architecture of Open vSwitch: divided into kernelspace and userspace}
\end{figure}

ovs-vswitchd, a daemon that implements the switch, along with a companion Linux kernel module for flow-based switching.
ovsdb-server, a lightweight database server that ovs-vswitchd queries to obtain its configuration.

The daemon which implements the switch is \textit{ovs-vswitchd} and it is shipped with an additional Linux kernel module for flow-based switching that it communicates with using the netlink protocol. The configuration for the switch is queried from a lightweight database server named \textit{ovsdb-server}.
Generally the decision about how a packet is processed is made in userspace, yet all following packets hit the cached entry in the kernel.

It is also possible to run it completely in userspace, but it decreases the performance drastically.

% %OpenStack - OVS Deep Dive % %


\subsubsection{OVSDB}
% %Open vSwtich Manual - DB config PDF % %

Each Open vSwitch daemon has a database (OVSDB) that holds it's configuration. The database is divided into multiple different tables with different purposes, with the ones related to this project outlined below:
\begin{itemize}
\item \textbf{Open\_vSwitch}: Open vSwitch configuration
\item \textbf{Port}: Port configuration
\item \textbf{Interface}: A physical network device within a Port
\item \textbf{QoS}: Quality of Service configuration
\item \textbf{Queue}: QoS output queue
\item \textbf{Controller}: OpenFlow controller configuration
\item \textbf{Manager}: OVSDB management connection
\end{itemize}


\subsubsection{OpenVSwitch Management}

Multiple different configuration utilities exist for OVS, however only ovs-vsctl is explained in this section. It is used for querying and updating the configuration of the switch through interaction with the ovsdb-server.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/fundamentals/openvswitch_vsctl.png}
\caption{Visualization of the interaction of the ovs-vsctl tool}
\end{figure}

Even the tool configures ovs-vswitchd, it can be seen as a high-level interface for the database.
The commands below are available for the basic OVS configuration that is needed to get it running for virtual network services:
\begin{itemize}
\item ovs-vsctl add-br \%bridge\%
\item ovs-vsctl list-br
\item ovs-vsctl add-port \%bridge\% \%port\%
\item ovs-vsctl list-ports \%bridge\%
\item ovs-vsctl get-manager \%bridge\%
\item ovs-vsctl get-controller \%bridge\%
\item ovs-vsctl list \%table\%
\end{itemize}


\subsubsection{QoS}

With Open vSwitch QoS can be configured for ports or the so-called virtual network interfaces that virtual machines get when they are connected to the internal bridge of the switch. The minimal and maximal rate-limits are defined in bytes and applied to a queue, which operates as an egress filter. The QoS port policies make use of the 'tc' implementation that is included in the Linux kernel.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/fundamentals/openvswitch_qos-queues.png}
\caption{QoS Queues attached to a Port in OVS}
\end{figure}


% % http://linux.die.net/man/8/tc % %
% % http://www.istudies.net/journal/sites/default/files/Extended%20Linux%20HTB%20Queuing%20Discipline%20Implementations.pdf % %
% %https://github.com/stanzgy/nova-network-qos/blob/master/nova-network-qos-intro.rst % %
Traffic control (tc) uses 'queueing discipline' (qdisc) for configuring the network interface. They are the fundamental schedulers used under Linux. When a packet is sent, it is enqueued to the qdisc for the interface and shortly after the kernel is trying to get as many packets as it can from the qdisc, so they can be forwarded to the network adaptor driver. By default the 'pfifo\_fast' qdisc is set in the kernel, which is a pure 'First In, First out' queue.

In OVS a classful qdisc named Hierarchy Token Bucket (HTB) is used for QoS. HTB allows guaranteeing bandwidth to classes with the possibility to define upper limits to inter-class sharing. Classes can also be prioritized.


\subsubsection{GRE}
% % http://www.juniper.net/documentation/en_US/junos14.1/topics/concept/gre-tunnel-services.html % %
Generic Routing Encapsulation (GRE) is used in OpenStack to tunnel the traffic between multiple nodes. It provides a private and secure path by encapsulating data packets.


\section{Cloud computing infrastructures}
% Call it Infrastructure-as-a-Service? %


\subsection{OpenStack}

% %OpenStack Operations Guide book % %
The OpenStack project was founded by Rackpace Cloud and NASA, however currently more than 200 companies are contributing.
With OpenStack one is able to design, deploy and maintain a private or public cloud. It is a flexible, scalable and open-source approach that combines multiple technologies into one Infrastructure-as-a-Service (IaaS). All of the interrelated services include an API that offers administrators different ways of controlling the cloud, be it through a web interface, a command-line client or a software development kit. All of the core components that come with OpenStack are implemented in Python.


\textbf{Conceptual architecture}
% %Juno Installation Guide PDF % %

The following graph shows the interaction between different OpenStack services that are involved in launching a virtual machine.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/fundamentals/openstack_conceptual_arch.png}
\caption{Interaction among OpenStack services}
\end{figure}


\subsection{OpenStack Compute (Nova)}

Nova is used to host and manage cloud computing systems. It supports different hypervisors and the number of physical hosts running the compute services can be scaled horizontally with no requirement of hardware resources from specific vendors. Hosts that provide Nova services are also called 'Compute Nodes'. Data center can be divided into so called tenants, which are isolated users with their own servers, security groups and externally reachable IP addresses (Floating IP addresses). 

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/fundamentals/openstack_nova.png}
\caption{OpenStack Compute service}
\end{figure}

% %https://kimizhang.wordpress.com/2013/08/26/openstack-zoning-regionavailability-zonehost-aggregate/ % %
\textbf{Compute Node segregation}

An OpenStack cloud can be logically and physically grouped on different levels:
\begin{itemize}
\item \textbf{Region:} A Region has its own full OpenStack deployment and can be physically at a different location. Regions share a set of Keystone and Horizon services to provide access control and the graphical management interface.
\item \textbf{Availability Zone:} Inside of a Region, it is possible to logically group multiple compute nodes into Availability Zones. This zone can be specified when new servers or stacks (via Heat) are instantiated.
\item \textbf{Host Aggregates:} Compute nodes can also be logically grouped into Host Aggregates by using meta-data to tag them. This feature can be used to separate nodes with certain hardware characteristics (e.g. with SSD drives) from others.
\end{itemize}

For zoning compute nodes availability zones will be used in the Connectivity Manager in order to achieve the best networking performance between individual servers.


\subsection{OpenStack Orchestration (Heat)}

Heat provides a template-based orchestration service for creating and managing cloud resources. This means multiple OpenStack resource types (such as virtual machines, floating IP addresses, volumes, security groups and users) can be generated and also maintained with additional functionality like auto-scaling.



\subsection{OpenStack Neutron}
% % PDF: Performance of Network Virtualization in CCI % %

In the early versions of OpenStack, virtual networking was a sub-component of Nova called Nova-network. This service had it's limitations, because it was closely coupled with networking abstractions and there were no APIs available. With Neutron the implementation is decoupled from the network abstraction and it provides a flexible management interface to administrators and users.


\subsubsection{Networking concepts}
% % http://docs.openstack.org/training-guides/content/operator-network-node.html % %

Neutron is responsible for defining network connectivity and addressing within OpenStack. In the main network abstraction the following components are defined. A network as a virtual layer 2 segment, a subnet as a layer 3 IP address space used within a network, a port as an interface to a network or subnet, a router that performs address translation and routing between subnets, a DHCP server responsible for IP adress distribution, a security group for filtering rules acting as a cloud firewall and Floating IPs to give VMs external network access.

Neutron exposes an extensible set of APIs for creating and managing those. Neutron consists of the following elements:
\begin{itemize}
\item \textbf{neutron-server:} Provides the logic for SDN and does not contain any SDN functionality in itself. It provides a generic API for the network operations, is modular and extended with the following agents.
\item \textbf{L2 agent:} Plugin-specific agent that manages networking on a compute node. For more details, see ML2 section.
\item \textbf{DHCP agent:} Provides DHCP services to tenant networks through dnsmasq instances.
\item \textbf{L3 agent:} Provides L3/NAT forwarding to allow external network access for VMs (virtual routers).
\item \textbf{Metadata agent:} Acts as a proxy to the metadata service of Nova
\end{itemize}

The different agents can interact with the neutron-server process through RPC and the OpenStack Networking API. In most use-cases the neutron-server and the different agents can run on the controller node or on a separate network controller node, however the plugin agent is running on each hypervisor.


\subsubsection{Modular Layer 2}

The ML2 plugin is a framework that allows the simultaneous usage of multiple layer 2 networking technologies.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/fundamentals/neutron_ml2.png}
\caption{Neutron modular framework, including ML2 drivers}
\end{figure}

The plugin interfaces with the type driver and the mechanism driver. The type driver defines the network types that can be declared when a new network is created and currently includes: local, flat, vlan, gre and vxlan. The mechanism driver specifies the mechanism for accessing these networks, i.e. Open vSwitch, Linux Bridge or other vendor-specific solutions. 



\textbf{ML2: Open vSwitch}

Open vSwitch is the ML2 mechanism driver that is set as default when installing using Devstack and is also the most commonly deployed agent.

% % Recreate this graphic!! % %
% % alternatively use neutron_gre_connection.png from http://docs.openstack.org/openstack-ops/content/network_troubleshooting.html % %
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/fundamentals/neutron_gre_connection_nodes.jpg}
\caption{GRE tunneling between Controller Node and Compute Node}
\end{figure}

\subsubsection{Network distinction}

Neutron is connected to different networks. For internet routable connections the external network is used. The management network is created by the network operator and is mapped to pre-existing networks within the datacenter, which is used to connect the different hosts. The tenants within OpenStack have their own isolated self provisioned private networks. Those can optionally be connected to other tenant or external networks. The abstraction of those tenant networks is possible through network namespaces. This allows overlapping IP addresses within the datacenter.


\subsubsection{Neutron workflow}

The workflow for Neutron, from starting to booting VMs is as follows:
\begin{enumerate}
\item Start Neutron-Server
\item Start Open vSwitch Agent
\item Start L3-Agent
\item Start DHCP-Agent
\item Start Metadata-Agent
\item Create Networks
\item Create Routers
\item Boot VMs
\end{enumerate}


\textbf{Neutron - Nova interaction}

\begin{enumerate}
\item Request: Create VM connected to network X (API)
\item Create VM (RPC: Nova API to Nova conductor)
\item Nova schedules VM
\item Create VM (RPC: Nova conductor to Nova compute)
\item Create Port (API: Nova compute to Neutron service)
\item Create tap device
\item Notify L2 agent (RPC)
\item get\_device\_details (RPC: L2 agent to Neutron service)
\item Configure local VLAN, OVS flows
\item Send port\_up notification (RPC: L2 agent to Neutron service)
\item Send port\_up notification (API: Neutron service to Nova)
\item port\_up (RPC: Nova service to Nova compute)
\item Nova compute boots VM

\end{enumerate}


\section{Conclusion}