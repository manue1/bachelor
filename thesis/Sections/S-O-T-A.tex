\chapter{State of the Art}

\section{Overview}

Three existing solutions for extending the services that Neutron provides with additional SDN features have been tested. They were selected based on the requirements given in the previous section.

\section{OpenDaylight SDN Controller}


OpenDaylight is fully implemented in Java. OpenDaylight exposes a single common OpenStack Service Northbound API which exactly matches the Neutron API. The OpenDaylight OpenStack Neutron Plugin simply passes through and therefore pushes complexity to OpenDaylight and simplifies the OpenStack plugin. The ML2 mechanism driver in Neutron has to be set to the OpenDaylight ML2 plugin, with the ODL agent running on the Compute Nodes. The OpenDaylight controller can be run on the Control Node or on a separate VM \cite{odl-intro}. The Open vSwitch database (OVSDB) Plugin component for OpenDaylight implements the OVSDB management protocol that allows the configuration of Open vSwitches. The OpenDaylight controller uses the native OVSDB implementation to manipulate the OVSDB. The component comprises a library and various plugins. The OVSDB protocol uses JSON/RPC calls to manipulate a physical or virtual switch that has OVSDB attached to it \cite{odl-ovsdb}.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/sota/odl_architecture.png}
\caption{Architecture of OpenDaylight Virtualization edition}
\end{figure}

The OVSDB component is accessible through a Northbound ReST API, which enables the operator to connect to the OpenFlow controller and modify various OVSDB tables. Through this API QoS rules can be deployed. Because it connects directly to the OpenVSwitch tables, all the QoS types that come with OpenVSwitch can be deployed (DSCP marking, setting priority, min-/max-rate for virtual network ports within OpenFlow Queues). In the local testbed it was possible to successfully deploy QoS rules on the ports of Virtual Machines. 


\section{Ryu SDN Controller}

Ryu is a component-based software defined networking framework which fully supports OpenFlow 1.0, 1.2, 1.3 and 1.4 switches and is fully written in Python \cite{ryu-start}. Ryu is a full featured OpenFlow controller that supports GRE and VLAN tunnelling. The OpenFlow controller that is embedded in the agent sets Flows on the switch by sending OpenFlow messages to the switch \cite{ryu-comparison}. It includes a set of apps which build the base of the SDN controller: L2 switch, ReST interface, topology viewer and tunnel modules. Ryu also includes an app that allows to set QoS rules through a ReST interface which uses a OVSDB interaction library to apply those. The QoS rules can be either applied to a specific Queue within a VLAN or a Switch port. It supports DSCP tagging and setting the min-rate and max-rate of an interface.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/sota/ryu_architecture.png}
\caption{Ryu architecture}
\end{figure}


As of OpenStack IceHouse Ryu makes use of the OFagent and is included in the Neutron core git repository. In order to use it as the SDN framework within Neutron, the OFagent  has to be set as both the ML2 mechanism driver (running on the control / network node) and the Neutron agent (running on the compute node). 


\section{OpenStack Neutron - QoS Extension}

A Neutron extension has been partially implemented for OpenStack IceHouse which includes an API for setting QoS on a per-tenant and per-port basis in combination with the Open vSwitch agent \cite{neutron-qos}. This approach makes a lot of sense as it extends the already existing Neutron API and the framework that is given for custom Neutron Extensions.



\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{images/sota/neutron_qos_extension.png}
\caption{Neutron QoS Extension architecture}
\end{figure}



\section{Problem Statement}

This section lists the restrictions that have been discovered with the previously mentioned solutions. 


\textbf{Problems encountered with OpenDaylight:}

The local testbed used for the integration of OpenStack Juno and OpenDaylight Helium consisted of 2 hosts, one running the OpenStack control node and OpenDaylight Controller and a OpenStack Compute Node on the second host. During the tests it was not possible to get the public network access for the Virtual Machines working, thus the L3 routing did not work. This and the fact that it ODL is very complex to debug and understand all underlying processes led us to the decision not to use OpenDaylight.


\textbf{Problems encountered with Ryu:}

The test of Ryu was unsuccessful due to a number of errors while stacking the test environment using Devstack. It was not possible to launch instances and test the QoS features. The lack of proper documentation for the interaction with OpenStack Neutron led us to look more into other SDN controllers for our particular use case. Currently Ryu doesn't support the Distributed Virtual Routing feature that has been introduced with OpenStack Juno.


\textbf{Problems encountered with Neutron QoS Extension:}

The implementation has not been finished and merged into Neutron, however the basic deployment of QoS seem to have been tested successfully. 

At the moment it is not clear if the OpenStack community will keep working on this. According to the code review comments, it was deferred to Juno, but it is not included in the current release and no active development is stated in the code / documentation platforms of the OpenStack community.

The patch consists of an extension to the Neutron API which allows setting QoS rules through the Neutron Python client, the actual Neutron extension with the QoS, QoS Driver in the Open vSwitch agent and an addition to the Neutron Database that includes QoS.

For the scope of this work, extending Neutron with QoS is not within the time frame of the project.

\newpage

\section{Conclusion}

The listed problems further strengthen the motivation for the implementation of the Connectivity Manager. Additionally, the second major requirement for the Optimal Virtual Machine Placement are not given in any of the outlined solutions. The following table lists a comparison and analysis of their features:




\begin{table}[H]
\centering

\begin{tabularx}{\textwidth}{ |X|X|X|X| }
\hline Tool/Solution & QoS support & OF Controller & OpenStack (Juno) integration \\ 
\hline Ryu & X & X & No (tests in Juno failed) \\ 
\hline OpenDaylight & X & X & No (tests in Juno failed) \\ 
\hline Neutron QoS extension & not fully implemented & No & No (IceHouse patch not ported to Juno) \\ 
\hline 
\end{tabularx}

\caption{State-of-the-art: Feature comparison}
\end{table}
