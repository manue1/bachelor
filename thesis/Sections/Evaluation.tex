\chapter{Evaluation}

\section{Feature analysis}



\section{Connectivity Manager integration with NUBOMEDIA project}

% % CM deliverable % %

The Connectivity Manager (CM) is part of the NUBOMEDIA platform and is placed between the virtual network resource management of the cloud infrastructure and the
multimedia application. The main focus of the CM is related to management and control of network functions of the virtual network infrastructure provided by OpenStack.

Nubomedia is an elastic Platform-as-a-Service (PaaS) cloud for interactive social multimedia. Its architecture is based on media pipelines: chains of elements providing media capabilities such as encryption, transcoding, augmented reality or video content analysis. These chains allow building arbitrarily complex media processing for applications. As a unique feature, from the point of view of the pipelines, the NUBOMEDIA cloud infrastructure behaves as a single virtual super-computer encompassing all the available resources of the underlying physical network.

\section{Network performance analysis}

The following section contains information about the used configuration and then different scenarios that were used to evaluate the effectiveness of the Connectivity Manager. All scenarios used the same topology and were run on a tenant without any other deployed servers or stacks. The bandwidth tests were performed using the iperf tool.

\subsection{Test-bed configuration}

The testbed consists of two nodes with the following hardware characteristics:

\begin{tabularx}{\textwidth}{ |X|X|X|X| }
\hline Name & \textbf{Controller node} & \textbf{Compute node} \\ 
\hline Hostname & datacenter-4 & dc4-comp \\ 
\hline OS & Ubuntu 14.04.1 LTS & Ubuntu 14.04.1 LTS \\ 
\hline RAM & 12 GB & 8 GB \\ 
\hline CPU & 8-core Intel Core i7-4765T CPU @ 2.00GHz & 4-core Intel(R) Core(TM) i3-2120T CPU @ 2.60GHz \\ 
\hline Ethernet card & Intel Corporation Gigabit Ethernet Connection I217-LM & Intel Corporation 82579LM Gigabit Network Connection \\ 
\hline 
\end{tabularx}

The two nodes are connected to a Gigabit-Ethernet switch. The installation of OpenStack was performed using the devstack script as outlined in section ... (Implementation / Devstack) .

\subsection{Installation of Connectivity Manager Agent}

A setup script exists in order to make it easier to get the CM Agent running. It builds installs all the necessary Python packages in a virtual environment, in order to have all packages isolated from the already existing Python set-up. This ensures that all packages are in the required version and don't interfere with the ones that are needed OpenStack or other applications. 

First of all the git repository needs to be cloned from the remote git server. For the installation the cm-agent.sh script needs to be executed with the 'install' option.
\begin{lstlisting}
stack@datacenter-4:~/nubomedia$ ./cm-agent.sh 
Usage: cm-agent.sh option
options:
  install   - install the server
  update    - updates the server
  start     - start the server
  uninstall - uninstall the server
  clean     - remove build files
\end{lstlisting}
The installation process includes setting up the virtual environment, installing all required Python packages and copying the configuration file to the /etc/nubomedia folder.

The configuration file needs to be customized, so it contains the IP address of the controller node and the correct OpenStack credentials:
\begin{lstlisting}
stack@datacenter-4:~$ cat /etc/nubomedia/cm-agent.properties 
os_username=admin
os_password=pass
os_auth_url=http://192.168.41.45:5000/v2.0
os_tenant=demo
\end{lstlisting}

Lastly it can be run in a screen session using the following command: \\
\textit{\$ venv/bin/python cm-agent/wsgi/application.py}

\subsection{Topology definition}

The topology that is used for the evaluation contains the following services instances:
\begin{lstlisting}
data/json_file/topologies/topology_local.json:
{
    "name":"local_nm_template_minified",
    "service_instances": [
        {
            "name":"Controller",
            "service_type":"Controller"
        },
        {
            "name":"Broker",
            "service_type":"Broker"
        },
        {
            "name":"MediaServer",
            "service_type":"MediaServer"
        }
    ]
}
\end{lstlisting}

It can be deployed using a test application which performs a HTTP POST to the EMM API at the \textit{/topologies} path.

The service types are further defined in another JSON file, which includes their configuration, networks and other parameters that are needed for provisioning. As one example the Media Server service is given below:

\begin{lstlisting}
data/json_file/services/MediaService.json 
{
    "service_type": "MediaServer",
    "version":"1",
    "image": "trusty-iperf",
    "flavor": "m1.mini",
    "key":"nubomedia",
    "configuration": {
    },
    "size": {
        "min": 1,
        "def": 3,
        "max": 5
    },
    "networks": [
        {
            "name":"Network-1",
            "private_net":"8048fd67-70a6-447d-a779-8a86f9eeb35d",
            "private_subnet": "0df3f54c-d1af-4b82-8376-18baa11d0e98",
            "public_net": "62024eab-23c2-4a81-a996-87af4d252282",
            "security_groups": [
                "SecurityGroup-MediaServer"
            ]
        }
    ],
    "requirements": [
        {
            "name":"$BROKER_IP",
            "parameter":"private_ip",
            "source":"Broker",
            "obj_name": "Network-1"
        }
    ]
}
\end{lstlisting}

\subsection{Scenario 1: Without Instance Placement Engine \& QoS enabled}

In this first scenario the deployment without any influence of the Connectivity Manager is shown. Here Nova randomly decides about the placement of the servers. In this case 4 servers were placed on the control node and one server on the compute node.

\begin{figure}[H]
\centering

\includegraphics[width=0.7\textwidth]{images/evaluation/testbed_scenario1}

\caption{Scenario 1: Placement of servers without Connectivity Manager}
\end{figure}

For testing the bandwidth the MediaServer-2 was used as a TCP server using iperf. All other servers connected to it in client mode sending and retrieving TCP packets in a timeframe of 10 seconds. The following graph shows the bandwidth usage of the servers.

\begin{figure}[H]
\centering

\includegraphics[width=0.5\textwidth]{images/evaluation/testbed_scenario1_bw}

\caption{Scenario 1: Bandwidth comparison}
\end{figure}

As visible the network performance of the server on the separate node performs much worse, which is also due to the fact that there is only a Gigabit-Ethernet connection between the nodes.

\subsection{Scenario 2: With Instance Placement Engine enabled, but without QoS}

In this next scenario, the Instance Placement Engine was enabled and therefore the availability zone in the topology was successfully set to a single host.

\begin{figure}[H]
\centering

\includegraphics[width=0.7\textwidth]{images/evaluation/testbed_scenario2}

\caption{Scenario 2: Placement of servers with Connectivity Manager}
\end{figure}

The following graph shows that the available bandwidth is now evenly distributed for all servers. However the traffic of the MediaServers should be prioritized, which is why QoS is needed to further improve the connectivity according to the requirements.

\begin{figure}[H]
\centering

\includegraphics[width=0.5\textwidth]{images/evaluation/testbed_scenario2_bw}

\caption{Scenario 2: Bandwidth comparison}
\end{figure}


\subsection{Scenario 3: Instance Placement Engine and QoS Manager enabled}

The servers are again placed on a single host and the Quality of Service configuration that was previously set in the configuration is applied. All media servers have a guaranteed bandwidth rate of 100 MBit/s and a maximum rate of 10 GBit/s, while all servers of other instance types have a rate between 100 MBit/s and 1 GBit/s.

\begin{figure}[H]
\centering

\includegraphics[width=0.7\textwidth]{images/evaluation/testbed_scenario3}

\caption{Scenario 3: Placement of servers using the Connectivity Manager with QoS enabled}
\end{figure}

For this last scenario two servers were set as iperf-servers and three servers as iperf-clients. The \textit{'Controller-1'} server acted as the server for a connection with the \textit{'MediaServer-2'} while the \textit{'MediaServer-3'} received packets from \textit{'MediaServer-1'} and \textit{'Broker-1'}.

\begin{figure}[H]
\centering

\includegraphics[width=0.5\textwidth]{images/evaluation/testbed_scenario2_bw}

\caption{Scenario 3: Bandwidth comparison}
\end{figure}

The first two bars show that the egress port on the Open vSwitch that the \textit{'MediaServer-3'} is connected to is limited to a combined bandwidth of 10 GBit/s. This is why the two connections share the available bandwidth and are nearly equal. It has to be remarked that the network performance is the average of a 10-second bandwidth test.

In the other QoS class the \textit{'MediaServer-2'} which is connecting to \textit{'Controller-1'} uses almost the full bandwidth of the available 1 GBit/s.

\section{Conclusion}

The three test scenarios show an improvement of the network performance, which is given through placing the servers on a single host. Additionally with enabling Quality of Service, the network bandwidth is shaped according to the requirements. It has been shown that the connection of MediaServers is prioritized to the connection of servers of other service instance types. The Service-Level-Agreement of the Gold and Wholesale classes are fulfilled.